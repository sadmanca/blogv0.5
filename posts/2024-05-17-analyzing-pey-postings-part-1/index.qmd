---
title: "Analyzing UofT PEY Co-op Job Postings - Part 1: Scraping Job Posting Data"
description: "Collecting unstructured data from the PEY Job Portal & pipelining it to a SQLite DB"
date: "2024-05-17"
categories: ["Writing"]
format:
  html:
    smooth-scroll: true
    code-tools: true
    code-copy: hover
embed-resources: true
filters:
  - quarto
  - line-highlight
jupyter: python3

reference-location: margin
citation-location: margin
image: resources/pey-part1.svg
---

Whether you're a new applicant to University of Toronto (UofT) Engineering/CS or someone who's going through their first, second, or even third year, you've probably been curious at one point or another about what jobs are actually posted on the much acclaimed PEY Co-op job board[^qns]. Well, as a computer engineering student at UofT who's just finished their third year (and consequently have been able to access the portal for these past two semesters), I thought it would be a good idea to do my own little data collection on the topic, along with maybe some analysis on reoccurring patterns[^quirks] (e.g. locations, international opportunities, etc.), and share both my findings and the raw data (hundreds of jobs, posted across several months) so that future PEY Co-op students can get a better idea of what they can expect from the portal (in a nutshell: better than you'd think, although it depends on who you exactly are).

[^qns]: Especially if you're not in the software industry, which for better or worse make up >70% of the jobs on the portal.

[^quirks]: There's also a couple of quirks that you might not know about if you don't already have access to the portal. 

# Scraping Job Posting Data

The UofT PEY Co-op job board itself is located behind a login portal at <https://www.uoftengcareerportal.ca/myAccount/internship/postings.htm>. To get in, you need to a) be a student at UofT (to have valid login credentials); b) be enrolled in the PEY Co-op program; and c) be registered to start your 12-16 month internship sometime between May and September following your current year (which means that unless you're a keen student in your second year who's opted in to get access and do your PEY early, you're either in your third year or you don't have access to the job portal). As an engineering student who's just finished their third year, I've had privileged access for 8 months and counting, and I've been able to save data on quite the number[^all] of job postings posted on the portal (but more on that later).

[^acc] Well, by the time you're reading this, I've finished my third year for quite some time.

[^all] And by "quite the number", I mean every single job posted since I've had access.

## What you can expect from the job board

The landing page for the PEY Co-op job board hosts the same look as the one for all the cross-institutional job boards at UofT's [CLNx](https://clnx.utoronto.ca/myAccount/dashboard.htm) and uses the same organization of elements and processes for browsing, searching for, and applying to job postings, so if you're a UofT student who's used CLNx in the past[^eg-ws] you already know what it's like to experience using the PEY job board.

[^eg-ws]: e.g. for applying to work study positions.

::: {.column-margin}
![The advanced search options available after clicking on `Search Job Postings`. Is *identical* to the job boards on CLNx.](resources/pey-advanced-search.jpg)

![Viewed jobs (after clicking on `Viewed`). Also *identical* to the job boards on CLNx.](resources/pey-viewed.jpg)
:::

![The PEY Co-op job board landing page. Note: you need to sign in using your UofT credentials before you can access this. Source: <https://www.uoftengcareerportal.ca/myAccount/internship/postings.htm>](resources/pey-landing-page.jpg)

### A sample job posting

Your average PEY job posting looks like the following:

::: {.column-body-outset}
::: {layout-ncol=2}
![Old design[^olddesign] for PEY job postings. One of the earliest jobs to be posted on the job board for my cohort in 2023.](resources/pey-old-posting.jpg)

[^olddesign]: This was the design used for a couple years, until the most recent redesign in late 2023 (which took place halfway through my fall semester of third year).

![Current design for PEY job postings. One of the most recent jobs posted at the time of writing.](resources/pey-sample-posting.jpg)
:::
:::

Nothing much to look at, just some basic tables with information about the job and the company, which thankfully should be simple to parse.

## Getting the posting date for jobs

One thing that's important to me since the very start of this project is making sure that the timestamps of job postings are available to view. As someone who took a few months before I landed a position that I was really satisified with, there were times where I felt a bit apprehensive at turning down offers for roles that I thought were fine but didn't feel excited about. I had no data beyond anecdotes from upper years about what's posted over the course of the fall and winter semesters, and so I couldn't really predict whether that dream role I had in mind was just a couple days from being posted (or whether jobs would start drying up so I should stick with whatever I had at that point in time), which is why I hope that at least one of the things this project of mine can provide is some reassurance to future PEY students about what jobs are posted and when[^but]. 

[^but]: One thing to keep in mind: every year is different, and just because some company posted some number of jobs at some point in time doesn't mean that they'll do it again next year. Of course, it also doesn't mean that they won't do it again, so make sure your takeaways from the data are taken with a grain of salt.

There's just one problem: there's absolutely no data indicating when a job was posted.

Well, except for one thing: the `New Posting Since Last Login` button on the landing page.

Whenever you login to the portal, that `New Posting Since Last Login` button gets updated with links to all of the jobs posted since your last login, so if you were to check the job board every single day and save the data for all of the job postings shown there each time you login, well, then you've successfully attributed a posting data for every single job.

Which is why that's exactly what I've done for the past 243 days (and counting).

### How I've been saving posting dates for jobs

And it's all thanks to Gildas Lormeau's[^gildas] [SingleFile web extension](https://github.com/gildas-lormeau/SingleFile)[^cli], which allows for saving a complete web page into a single HTML file (unlike Chrome[^chrome]). In addition (and rather importantly), the SingleFile extension allows for saving pages for *all tabs in the current window* (this is important for making the whole archival process a not-headache).

[^gildas]: https://github.com/gildas-lormeau

[^cli]: While there's also a CLI tool available, the tricky navigation for the PEY job board website means that manually navigating to pages & then saving them using the extension is a lot easier.

[^chrome]: Chrome and virtually all other browsers have a slightly more complicated setup for saving pages which makes organizing files for saved pages slightly less elegant compared to dealing with just a single file via SingleFile: when you press `Ctrl+S` on a page, it doesn't just save that page's HTML file but also a folder containing all of the media from the page (which, given that none of the job postings contain images, is just one more thing to have to delete).

By CTRL-clicking on every single job posting shown behind `New Posting Since Last Login` (so that every new posting opens on a new tab) and then using the SingleFile extension to save the page each tab in one go[^ext], I'm able to condense the whole process of saving new postings for the day to just 1-2 minutes. Put each day's postings into a timestamped folder (made faster thanks to a handy AutoHotKey script that's always a keyboard shortcut away), which itself goes into a big folder on my local computer of all PEY job postings collected thus far, and I've got myself data on over 2000 job postings just waiting to be analyzed for some insights.

```
- üìÅPEY POSTINGS ARCHIVE/
  - üìÅ2023-09-17_20-14-10/
    - UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (9_17_2023 8_13_11 PM).html
    - UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (9_17_2023 8_13_12 PM).html
    - ...
  - üìÅ2023-09-18_00-51-40/
    - Job ID_ _ 43541 UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (9_18_2023 12_51_48 AM).html
    - Job ID_ _ 43554 UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (9_18_2023 12_52_08 AM).html
    - ...
  - üìÅ2023-09-18_16-08-36/
  - üìÅ2023-09-18_16-08-36/
  - üìÅ2023-09-22_20-29-46/
  - üìÅ2023-09-25_20-16-32/
  - ...
  - ...
  - ...
  - üìÅ2024-05-16_11-24-45/
  - üìÅ2024-05-16_23-55-40/
  - üìÅ2024-05-17_11-58-36/
    - 48007 -_ _ _ _ _ Software Developer Intern (Fall_September 2024, 8-16 Months) - UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (2024-05-17 11_57_42 AM).html
    - 48013 -_ _ _ _ _ Solutions Engineering Intern (Fall_September 2024, 8-12 Months) - UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (2024-05-17 11_57_42 AM).html
    - 48017 -_ _ _ _ _ Research & Technology - Engineering Program Management Intern - UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (2024-05-17 11_57_42 AM).html
  - üìÅ2024-05-17_14-12-49/
  - üìÅ2024-05-17_16-25-26/
```

<<<marginnote about how there have been many changes over time, including the ecc website redesign, my changing of how files are named in singlefile by default>>>

[^ext]: Using the `Save all tabs` option under the SingleFile extension.

### Why not write a script to automate saving postings?

Is it possible to automate the whole process of saving data for job postings? Technically, yes, it's absolutely feasible, but given how easy it is manually save data for job postings in a minute or two for every couple hundred of jobs (with the assistance of a few scripts to make the CTRL-clicking a lot faster), it's just not worth the time to make the routine task more efficient[^xkcd1] (I'd be spending more time than I'd save, as any XKCD enjoyer can relate to[^xkcd2]).

[^xkcd1]: [Automation](https://imgs.xkcd.com/comics/is_it_worth_the_time.png)
[^xkcd2]: [Is It Worth the Time?](https://imgs.xkcd.com/comics/automation.png)

# Loading job posting data to a database

HTML is fine for temporary storage purposes[^issok], but I need something that will allow me to view and analyze the data in a fast, efficient, and easy-to-use manner. Enter the underrated gem of database technologies: SQLite.

[^issok]: Despite a large portion of the saved HTML files conssiting of useless space in lines of JavaScript, CSS, and HTML that have nothing to do with the data that counts, it's just not worth the time investment for me to parse through every file and remove the clutter since all the saved job postings for a year only take up a couple hundred MB.

Now, I've never used SQLite before[^psql], but thanks to the [`sqlite3` documentation for Python](https://docs.python.org/3/library/sqlite3.html) it looks like I'll be right-at-home as someone who's used `psycopg2` in the past[^whypy].

[^psql]: Just PostgreSQL, with a bit of Python on the side via `psycopg2`.

[^whypy]: And of course Python is ideal for the scale of data I'm working with here, just a couple thousand HTML files.

But before I can start inputting all the data into a sqlite database, I need to figure out how to extract the key information (e.g. job title, location, description, company etc.) first.

## Extracting data from HTML 

If it were just a single page, I could use something like [Microsoft Edge's Smart Copy](https://techcommunity.microsoft.com/t5/discussions/smart-copy-is-available-in-edge-now/m-p/1909748) or the [Table Capture extension](https://chromewebstore.google.com/detail/table-capture/iebpjdmgckacbodjpijphcplhebcmeop) and call it a day, but extracting data from >20k pages is a whole different ballgame.

The HTML code for each job posting page doesn't have the best formatting[^emptylines], but thanks to everything being stored in tables I can just use Python's trusty BeautifulSoup4 library on my locally saved HTML pages and get the text in every table data cell in less than 50 lines of code:

[^emptylines]: 

  The page Nanoleaf position above has 3506 empty lines with only 809 lines of actual HTML/CSS/JS code, and the HTML that *does* exist isn't exactly the most pleasing to read:
  
  ```{.html filename="Job ID_ _ 43628 UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (9_18_2023 12_52_04 AM).html"}
  <div class=orbisModuleHeader>
  <div class=row-fluid>
  <div class=span2>
  <div style=text-align:center>
  <h1>
  Job ID
  : 43628
  </h1>
  </div>
  </div>
  <div class=span7>
  <h1>
  ML Developer - Software Automation &amp; Developer Infrastructure
  </h1>
  <h5> 
  
  Cerebras Systems - Computer Science 
  </h5>
  </div>
  <div class=span3>
  
  <ul class=pager>
  
  <li>
  <a href=javascript:void(0)><i class=icon-chevron-left></i>Back to Overview</a>
  </li>
  ```

```{.python filename="parse.py"}
import argparse
from bs4 import BeautifulSoup

def parse_html_file(filepath, verbose=False):
    with open(filepath, 'r', encoding='utf-8') as file:
        html_content = file.read()

    soup = BeautifulSoup(html_content, 'lxml')

    data = {}
    rows = soup.find_all('tr')  # find all table rows

    for row in rows:
        tds = row.find_all('td')  # find all table data cells
        
        if len(tds) >= 2:
            label_td = tds[0]
            label_text = '\n'.join(label_td.stripped_strings)
            
            value_td = tds[1]
            value_text = '\n'.join(value_td.stripped_strings)
            
            data[label_text] = value_text

    if verbose:
        for key, value in data.items():
            print(f"{key}: {value}")
    else:
        print("Parsing completed.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Parse HTML for 2 column table data.")
    parser.add_argument("-f", "--filepath", required=True, help="Path to the HTML file to be parsed.")
    parser.add_argument("-v", "--verbose", action="store_true", help="Print parsed data.")

    args = parser.parse_args()
    parse_html_file(args.filepath, args.verbose)
```

::: {.column-page}
::: columns
::: {.column width="40%"}

![](resources/pey-old-posting.jpg){height=50%}
:::

::: {.column width="60%"}

```{.bash}
> python parse.py --verbose --filepath "Job ID_ _ 43628 UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (9_18_2023 12_52_04 AM).html"

Position Type:: Professional Experience Year Co-op (PEY Co-op: 12-16 months)
Job Title:: ML Developer - Software Automation & Developer Infrastructure
Job Location:: Toronto
Job Location Type:: Flexible
If working on site, can you provide a copy of your COVID-19 safety protocols?:: No
Number of Positions:: 1
Salary:: $42.00 hourly for 40.0 hours per week
Start Date:: 05/06/2024
End Date:: 04/25/2025
Job Function:: Information Technology (IT)
Job Description:: Cerebras Systems has pioneered a groundbreaking chip and system that revolutionizes deep learning applications. Our system empowers ML researchers to achieve unprecedented speeds in training and inference workloads, propelling AI innovation to new horizons.
The Condor Galaxy 1 (CG-1), unveiled in a recent announcement, stands as a testament to Cerebras' commitment to pushing the boundaries of AI computing. With a staggering 4 ExaFLOP processing power, 54 million cores, and 64-node architecture, the CG-1 is the first of nine powerful supercomputers to be built and operated through an exclusive partnership between Cerebras and G42. This strategic collaboration aims to redefine the possibilities of AI by creating a network of interconnected supercomputers that will collectively deliver a mind-boggling 36 ExaFLOPS of AI compute power upon completion in 2024.
Cerebras is building a team of exceptional people to work together on big problems. Join us!.
About The Role
As a Machine Developer - Software Automation & Developer Infrastructure Engineer, you will use your knowledge of testing and testability to influence better software design, promote proper engineering practice, bug prevention strategies, testability, scalability, and other advanced quality concepts. The position will play a huge role in the quality of Cerebras software. We are looking for engineers that have a broad set of technical skills and who are ready to tackle the biggest at-scale problems in HW-based deep learning accelerators.
Responsibilities
Write scripts to automate testing and create tools to allow easy development of software regression tests
Help identify weak spots and potential customer pain points and drive the software organization towards customer focused quality metrics
Implement creative ways to break software and identify potential problems
Contribute to developing requirements specifications with a focus on developing verification tests
Job Requirements:: Requirements
Enrolled within University of Toronto's PEY program with a degree in Computer Science, Computer Engineering, or any other related discipline
Experience in developing automated tests for compute/machine learning or networking systems within a large-scale enterprise environment
Ability to take responsibility for monitoring product development and usage at all levels with an end goal toward improving product quality
Strong knowledge of software system design, C++ and Python
Preferred
Strong software testing experience with a proven track record in scaling highly technical teams
Knowledge of UNIX/Linux and Windows environments
Knowledge of neural network architecture and ML/AI deep learning principles
Prior experience in designing and developing test automation for HW systems involving ASICs or FPGAs
Prior experience working with live hardware systems and debug tools operating in a real time environment such as networking devices or live computing systems
Preferred Disciplines:: Computer Engineering
Computer Science
Engineering Science (Electrical and Computer)
Engineering Science (Infrastructure)
Engineering Science (Machine Intelligence)
Engineering Science (Robotics)
All Co-op programs:: No
Targeted Co-op Programs:: Targeted Programs
Professional Experience Year Co-op (12 - 16 months)
Application Deadline:: Nov 1, 2023 11:59 PM
Application Receipt Procedure:: Online via system
If by Website, go to:: https://www.cerebras.net/careers/?gh_jid=5321500003
Additional Application Information:: Please apply with
both resume & transcript.
Lacking transcript will disqualify you from being considered.
Note that applications will be considered on a rolling basis. Apply as early as possible.
Note to PEY Co-op applicants: In addition to your application by email/website, please ensure that you select the ‚ÄúI intend to apply for this position‚Äù tab on the portal. ¬†This will give us a record of your submitted application in the event that you will be invited for interviews.
U of T Job Coordinator:: Yasmine Abdelhady
Organization:: Cerebras Systems
Division:: Computer Science
Website:: https://cerebras.net/
Length of Workterm:: FLEXIBLE PEY Co-op: 12-16 months (range)
```

:::
:::
:::

### Finetuning data extraction 

There's a few nuances to the data extraction that mean this simple script needs *just* a bit more extending so it can properly parse the entire local dataset. One nuance is the fact that the formatting for HTML pages has changed[^attimes] (several times actually) over the course of the last two semesters. Since the location of the data has always remained in tables, that's largely a non-issue. While the job title and company name included in the header above any table on job posting pages are currently missed by the script, that data is also present in the tables below (and extracted by the script properly), so that, too, is a non-issue. With one exception: job IDs aren't extracted. Luckily, I had the foresight to configure SingleFile to include the job ID automatically as part of the filename for each HTML page[^except] back when I started the archival process, so I can add some logic to parse that too and nothing of value is missed.

[^except]: Except for the first 5 job postings I ever saved (which I can just manually edit filenames for to ensure that the script parses them properly).

[^attimes]: At times purposefully by myself (e.g. adding Job ID and later Job Title to filenames for saved HTML files for easier file browsing and duplication checking), and at times by the university (i.e. there was a big redesign of the job board that took place in the latter half of the fall semester, which made it so the PEY job board uses the same frontend design as job boards on CLNx, whereas prior to that it looked a bit different despite functionally working the same).

Now with a method of easily extracting all the relevant data from each HTML page (the job posting ID can just be retrieved from the filename), all that's left is to automate running the parser across all files saved within timestamped subdirectories on my local computer and pipe the data into a sqlite db.

## Storing data in SQLite

idk

====================
This isn't the first time archiving job postings has come across my mind. In my first year of university, I was *pretty* bewildered at just how many links I had access to with resources of all kinds (now that I've been able to actually look through many of them, I realize that they're not all quite as useful as one would hope). Like any enterprising student, there was one concern that persisted above all: how the heck can I get a job?

Thankfully for all of us UofT students, the University of Toronto has it's university-wide job board (and much more) in the form of....

AUTHOR'S NOTE: cut this short, discussion for a later date
====================