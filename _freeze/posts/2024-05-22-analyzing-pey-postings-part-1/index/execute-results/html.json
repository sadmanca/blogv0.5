{
  "hash": "6850f266bcaf0ce95f69e7640e5b680a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Analyzing UofT PEY Co-op Job Postings (2023-2024) - Part 1: Scraping Job Board Data'\ndescription: Collecting unstructured data on ~2k job postings from UofT's PEY Co-op job board & pipelining it to a single SQLite DB less than 10MB in size\ndate: '2024-05-22'\ncategories:\n  - Writing\nformat:\n  html:\n    smooth-scroll: true\n    code-tools: true\n    code-copy: hover\nembed-resources: false\nfilters:\n  - quarto\n  - line-highlight\n  - interactive-sql\ndatabases:\n  - name: job_postings\n    path: 'https://raw.githubusercontent.com/sadmanca/uoft-pey-coop-job-postings/raw/job_postings.sql'\nreference-location: margin\ncitation-location: margin\nimage: resources/pey-part1.svg\n---\n\nWhether you're a new applicant to engineering/computer science at the University of Toronto (UofT) or someone who's going through their first, second, or even third or fourth years, you've probably been curious at one point or another about what jobs are actually posted on the much acclaimed[^welltosome] PEY Co-op job board[^qns] for 12-16 month positions .\n\n[^welltosome]: well, to some, at least\n\nWell, as a computer engineering student at UofT who's just finished their third year (and who's consequently been able to access the portal for the past two semesters), I thought it would be interesting to my own little data collection on the topic, along with some analysis on recurring patterns[^quirks] (e.g. locations, international opportunities, etc.), and share both my findings and the raw data (thousands of jobs posted across several months) so that future PEY Co-op students can get a better idea of what they can expect from the portal[^nutshell].\n\n[^nutshell]: In a nutshell: better than you'd think, although it depends on who you exactly are. In some cases, it can be a *lot* worse.\n\n[^qns]: Especially if you're not in the software industry, which for better or worse make up >70% of the jobs on the portal.\n\n[^quirks]: There's also a couple of quirks that you might not know about if you don't already have access to the portal.\n\n# Scraping job posting data\n\nThe UofT PEY Co-op job board itself is located behind a login portal at [uoftengcareerportal.ca/myAccount/internship/postings.htm](https://uoftengcareerportal.ca/myAccount/internship/postings.htm). To get in, you need to a) be a student at UofT (to have valid login credentials); b) be enrolled in the PEY Co-op program; and c) be registered to start your 12-16 month internship sometime between May and September following your current year (which means that unless you're a keen student in your second year who's opted in to get access and do your PEY early, you're either in your third year or you don't have access to the job portal). As an engineering student who's just finished their third year[^acc], I've had privileged access for 8 months and counting, and I've been able to save data on quite the number[^all] of job postings posted on the portal (but more on that later).\n\n[^acc]: Well, by the time you're reading this, it'll have been a while since I finished my third year.\n\n[^all]: And by \"quite the number\", I mean every single job posted since I've had access.\n\n## What you can expect from the job board\n\nThe [landing page](https://uoftengcareerportal.ca/myAccount/internship/postings.htm)[^access] for the PEY Co-op job board hosts the same look as the one for all the cross-institutional job boards at UofT's [CLNx](https://clnx.utoronto.ca/myAccount/dashboard.htm) and uses the same organization of elements and processes for browsing, searching for, and applying to job postings, so if you're a UofT student who's used CLNx in the past[^eg-ws] you already know what it's like to experience using the PEY job board.\n\n[^eg-ws]: e.g. for applying to work study positions.\n\n[^access]: Note: you need to sign in using your UofT credentials before you can access this.\n\n::: {.column-page}\n\n:::: {.columns}\n\n::: {.column width=\"52%\"}\n![The PEY Co-op job board landing page (`Job Postings`).](resources/pey-landing-page.jpg){width=500}\n\n![Viewed jobs (`Viewed`). Note: the page shows that I've viewed 2164 jobs on the portal, even though there were only ~1.8k PEY Co-op positions ever posted (as you'll learn later on). ***Why's that?*** Well, the remaining ~300 jobs are actually from when the portal opened up for my cohort's [PEY Co-op **summer** work-term (12-16 weeks)](https://engineeringcareers.utoronto.ca/pey-co-op-overview/). I'll leave a dive into summer job postings for sometime in the future.](resources/pey-viewed.jpg){width=700}\n:::\n\n::: {.column width=\"2%\"}\n\n:::\n\n::: {.column width=\"46%\"}\n![Advanced search options (`Search Job Postings`).](resources/pey-advanced-search.jpg){width=800}\n:::\n\n::::\n\n:::\n\n### Sample job postings\n\nYour average PEY job postings look like the below. *Note:* The design on the left was used for a couple years, until the most recent redesign in late 2023 (which took place halfway through my fall semester of third year). If you're an upcoming PEY Co-op student, job postings should look like the right for you.\n\n::: {.column-page}\n\n:::: {.columns}\n\n::: {.column width=\"56%\"}\n![Old design for PEY job postings. One of the earliest jobs to be posted on the job board for my cohort (2T5) in 2023. Shown: `ML Developer - Software Automation & Developer Infrastructure` @ `Cerebras`](resources/pey-old-posting.jpg){width=500}\n:::\n\n::: {.column width=\"1%\"}\n\n:::\n\n::: {.column width=\"43%\"}\n![Current design for PEY job postings. One of the most recent jobs posted at the time of writing. Shown: `Software Developer Intern` @ `Geotab`](resources/pey-sample-posting.jpg){width=550}\n:::\n\n::::\n\n:::\n\nNothing much to look at, just some basic tables with information about the job and the company, which thankfully are simple to parse.\n\n## Getting the posting date for jobs\n\nOne thing that's been important to me since the very start of this project is making sure that the timestamps of job postings are available to view. As someone who took a few months before I landed a position that I was really satisified with, there were times where I felt a bit apprehensive at turning down offers for roles that I thought were fine but didn't feel excited about. I had no data beyond anecdotes from upper years about what's posted over the course of the fall and winter semesters, and so I couldn't really predict whether that dream role I had in mind was just a couple days from being posted (or whether jobs would start drying up so I should stick with whatever I had at that point in time), which is why I hope that at least one of the things this project of mine can provide is some reassurance to future PEY students about what jobs are posted and when[^but].\n\n[^but]: One thing to keep in mind: every year is different, and just because some company posted some number of jobs at some point in time doesn't mean that they'll do it again next year. Of course, it also doesn't mean that they won't do it again, so make sure your takeaways from the data are taken with a grain of salt.\n\nThere's just one problem: there's absolutely no data indicating when a job was posted.\n\nWell, except for one thing: the `New Posting Since Last Login` button on the landing page.\n\nWhenever you login to the portal, that `New Posting Since Last Login` button gets updated with links to all of the jobs posted since your last login, so if you were to check the job board every single day and save the data for all of the job postings shown there each time you login, well, then you've successfully attributed a posting data for every single job.\n\nWhich is why that's exactly what I've done for the past 243 days (and counting).\n\n### How I've been saving posting dates for jobs\n\nAnd it's all thanks to Gildas Lormeau's[^gildas] [SingleFile web extension](https://github.com/gildas-lormeau/SingleFile)[^cli], which allows for saving a complete web page into a single HTML file (unlike Chrome[^chrome]). In addition (and rather importantly), the SingleFile extension allows for saving pages for *all tabs in the current window* (this is important for making the whole archival process a not-headache).\n\n[^gildas]: [github.com/gildas-lormeau](https://github.com/gildas-lormeau)\n\n[^cli]: While there's also a CLI tool available, the tricky navigation for the PEY job board website means that manually navigating to pages & then saving them using the extension is a lot easier.\n\n[^chrome]: Chrome and virtually all other browsers have a slightly more complicated setup for saving pages which makes organizing files for saved pages slightly less elegant compared to dealing with just a single file via SingleFile: when you press `Ctrl+S` on a page, it doesn't just save that page's HTML file but also a folder containing all of the media from the page (which, given that none of the job postings contain images, is just one more thing to have to delete).\n\nBy `CTRL-click`ing on every single job posting shown behind `New Posting Since Last Login` (so that every new posting opens on a new tab) and then using the SingleFile extension to save the page each tab in one go[^ext], I'm able to condense the whole process of saving new postings for the day to just 1-2 minutes. Put each day's postings into a timestamped folder (made faster thanks to a handy AutoHotKey script that's always a keyboard shortcut away), which itself goes into a big folder on my local computer of all PEY job postings collected thus far, and I've got myself data on almost 2k job postings just waiting to be analyzed for some insights.\n\n::: {#fig-dir}\n```\nüìÅPEY POSTINGS ARCHIVE/\n‚îú‚îÄ‚îÄ üìÅ2023-09-17_20-14-10/\n‚îÇ   ‚îú‚îÄ‚îÄ UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (9_17_2023 8_13_11 PM).html\n‚îÇ   ‚îú‚îÄ‚îÄ UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (9_17_2023 8_13_12 PM).html\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ üìÅ2023-09-18_00-51-40/\n‚îÇ   ‚îú‚îÄ‚îÄ Job ID_ _ 43541 UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (9_18_2023 12_51_48 AM).html\n‚îÇ   ‚îú‚îÄ‚îÄ Job ID_ _ 43554 UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (9_18_2023 12_52_08 AM).html\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ üìÅ2023-09-18_16-08-36/\n‚îú‚îÄ‚îÄ üìÅ2023-09-18_16-08-36/\n‚îú‚îÄ‚îÄ üìÅ2023-09-22_20-29-46/\n‚îú‚îÄ‚îÄ üìÅ2023-09-25_20-16-32/\n‚îú‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ üìÅ2024-05-16_11-24-45/\n‚îú‚îÄ‚îÄ üìÅ2024-05-16_23-55-40/\n‚îú‚îÄ‚îÄ üìÅ2024-05-17_11-58-36/\n‚îÇ   ‚îú‚îÄ‚îÄ 48007 -_ _ _ _ _ Software Developer Intern (Fall_September 2024, 8-16 Months) - UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (2024-05-17 11_57_42 AM).html\n‚îÇ   ‚îú‚îÄ‚îÄ 48013 -_ _ _ _ _ Solutions Engineering Intern (Fall_September 2024, 8-12 Months) - UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (2024-05-17 11_57_42 AM).html\n‚îÇ   ‚îî‚îÄ‚îÄ 48017 -_ _ _ _ _ Research & Technology - Engineering Program Management Intern - UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (2024-05-17 11_57_42 AM).html\n‚îú‚îÄ‚îÄ üìÅ2024-05-17_14-12-49/\n‚îî‚îÄ‚îÄ üìÅ2024-05-17_16-25-26/\n```\n\nA snapshot of my local directory structure with HTML pages for saved job postings. You might notice that the naming convention for HTML files are different across the 3 folders shown. That's because my file naming system did indeed change 3 times over the course of my two semesters, including due to the job board redesign and my reconfiguring of options for SingleFile.\n\n:::\n\n[^ext]: Using the `Save all tabs` option under the SingleFile extension.\n\n### Why not write a script to automate saving postings?\n\nIs it possible to automate the whole process of saving data for job postings? Technically, yes, it's absolutely feasible, but given how easy it is manually save data for job postings in a minute or two for every couple hundred of jobs (with the assistance of a few scripts to make the `CTRL-click`ing a lot faster), it's just not worth the time to make the routine task more efficient[^xkcd1] (I'd be spending more time than I'd save, as any XKCD enjoyer can relate to[^xkcd2]).\n\n[^xkcd1]: [![xkcd: \"Automation\"](https://imgs.xkcd.com/comics/is_it_worth_the_time.png)](https://xkcd.com/1205/)\n[^xkcd2]: [![xkcd: \"Is It Worth the Time?\"](https://imgs.xkcd.com/comics/automation.png)](https://xkcd.com/1319/)\n\n# Storing job postings in a database\n\nHTML is fine for temporary storage purposes[^issok], but I need something that will allow me to view and analyze the data in a fast, efficient, and easy-to-use manner. Enter the underrated gem of database technologies: SQLite.\n\n[^issok]: Despite a large portion of the saved HTML files consisting of useless space in lines of JavaScript, CSS, and HTML that have nothing to do with the data that counts, it's just not worth the time investment for me to parse through every file and remove the clutter since all the saved job postings for a year only take up a couple hundred MB.\n\nNow, I've never used SQLite before[^psql], but thanks to the [`sqlite3` documentation for Python](https://docs.python.org/3/library/sqlite3.html) it looks like I'll be right-at-home as someone who's used `psycopg2` in the past[^whypy].\n\n[^psql]: Just PostgreSQL, with a bit of Python on the side via `psycopg2`.\n\n[^whypy]: And of course Python is ideal for the scale of data I'm working with here, just a couple thousand HTML files.\n\nBut before I can start inputting all the data into a SQLite database, I need to figure out how to extract the key information (e.g. job title, location, description, company etc.) first.\n\n## Extracting data from HTML\n\nIf it were just a single page, I could use something like [Microsoft Edge's Smart Copy](https://techcommunity.microsoft.com/t5/discussions/smart-copy-is-available-in-edge-now/m-p/1909748) or the [Table Capture extension](https://chromewebstore.google.com/detail/table-capture/iebpjdmgckacbodjpijphcplhebcmeop) and call it a day, but extracting data from >20k pages is a whole different ballgame.\n\nThe HTML code for each job posting page doesn't have the best formatting[^emptylines], but thanks to everything being stored in tables I can just use Python's trusty BeautifulSoup4 library on my locally saved HTML pages and get the text in every table data cell in less than 50 lines of code:\n\n[^emptylines]:\n\n    The page for the Cerebras position above has 3506 empty lines with only 809 lines dedicated to actual code, and the HTML code on non-empty lines isn't exactly the most pleasing to read:\n\n\n    ```{=html}\n    <p style=\"margin-bottom:5pt;\"></p>\n    ```\n\n\n    ```{.html filename=\"sample_job.html\"}\n    <!-- Job ID_ _ 43628 UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (9_18_2023 12_52_04 AM).html -->\n\n    <div class=orbisModuleHeader>\n    <div class=row-fluid>\n    <div class=span2>\n    <div style=text-align:center>\n    <h1>\n    Job ID\n    : 43628\n    </h1>\n    </div>\n    </div>\n    <div class=span7>\n    <h1>\n    ML Developer - Software Automation &amp; Developer Infrastructure\n    </h1>\n    <h5>\n\n    Cerebras Systems - Computer Science\n    </h5>\n    </div>\n    <div class=span3>\n\n    <ul class=pager>\n\n    <li>\n    <a href=javascript:void(0)><i class=icon-chevron-left></i>Back to Overview</a>\n    </li>\n\n    <!-- ... -->\n    ```\n\n```{.python filename=\"parse.py\"}\nimport argparse\nfrom bs4 import BeautifulSoup\n\ndef parse_html_file(filepath, verbose=False):\n    with open(filepath, 'r', encoding='utf-8') as file:\n        html_content = file.read()\n\n    soup = BeautifulSoup(html_content, 'lxml')\n\n    data = {}\n    rows = soup.find_all('tr')  # find all table rows\n\n    for row in rows:\n        tds = row.find_all('td')  # find all table data cells\n\n        if len(tds) >= 2:\n            label_td = tds[0]\n            label_text = '\\n'.join(label_td.stripped_strings)\n\n            value_td = tds[1]\n            value_text = '\\n'.join(value_td.stripped_strings)\n\n            data[label_text] = value_text\n\n    if verbose:\n        for key, value in data.items():\n            print(f\"{key}: {value}\")\n    else:\n        print(\"Parsing completed.\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Parse HTML for 2 column table data.\")\n    parser.add_argument(\"-f\", \"--filepath\", required=True, help=\"Path to the HTML file to be parsed.\")\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Print parsed data.\")\n\n    args = parser.parse_args()\n    parse_html_file(args.filepath, args.verbose)\n```\n\n\n```{=html}\n<p style=\"margin-bottom:2.5cm;\"></p>\n```\n\n\nTrying out the quickly-written `parse.py` script on one of the saved HTML pages, and it's able to get all of the values for all of the table fields with no issues:\n\n::: {.column-body-outset}\n:::: columns\n::: {.column width=\"54%\"}\n\n![](resources/pey-old-posting.jpg){width=550}\n:::\n\n::: {.column width=\"1%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n\n```{.bash}\n$ python parse.py --verbose --filepath \"Job ID_ _ 43628 UofT Engineering Career Centre - MyAccount - Co-op & Experience Programs - Job Postings (9_18_2023 12_52_04 AM).html\"\n```\n::: {style=\"font-size: 75%;\"}\n```{.txt .code-overflow-wrap}\nPosition Type:: Professional Experience Year Co-op (PEY Co-op: 12-16 months)\nJob Title:: ML Developer - Software Automation & Developer Infrastructure\nJob Location:: Toronto\nJob Location Type:: Flexible\nIf working on site, can you provide a copy of your COVID-19 safety protocols?:: No\nNumber of Positions:: 1\nSalary:: $42.00 hourly for 40.0 hours per week\nStart Date:: 05/06/2024\nEnd Date:: 04/25/2025\nJob Function:: Information Technology (IT)\nJob Description:: Cerebras Systems has pioneered a groundbreaking chip and system that revolutionizes deep learning applications. Our system empowers ML researchers to achieve unprecedented speeds in training and inference workloads, propelling AI innovation to new horizons.\nThe Condor Galaxy 1 (CG-1), unveiled in a recent announcement, stands as a testament to Cerebras' commitment to pushing the boundaries of AI computing. With a staggering 4 ExaFLOP processing power, 54 million cores, and 64-node architecture, the CG-1 is the first of nine powerful supercomputers to be built and operated through an exclusive partnership between Cerebras and G42. This strategic collaboration aims to redefine the possibilities of AI by creating a network of interconnected supercomputers that will collectively deliver a mind-boggling 36 ExaFLOPS of AI compute power upon completion in 2024.\nCerebras is building a team of exceptional people to work together on big problems. Join us!.\nAbout The Role\nAs a Machine Developer - Software Automation & Developer Infrastructure Engineer, you will use your knowledge of testing and testability to influence better software design, promote proper engineering practice, bug prevention strategies, testability, scalability, and other advanced quality concepts. The position will play a huge role in the quality of Cerebras software. We are looking for engineers that have a broad set of technical skills and who are ready to tackle the biggest at-scale problems in HW-based deep learning accelerators.\nResponsibilities\nWrite scripts to automate testing and create tools to allow easy development of software regression tests\nHelp identify weak spots and potential customer pain points and drive the software organization towards customer focused quality metrics\nImplement creative ways to break software and identify potential problems\nContribute to developing requirements specifications with a focus on developing verification tests\nJob Requirements:: Requirements\nEnrolled within University of Toronto's PEY program with a degree in Computer Science, Computer Engineering, or any other related discipline\nExperience in developing automated tests for compute/machine learning or networking systems within a large-scale enterprise environment\nAbility to take responsibility for monitoring product development and usage at all levels with an end goal toward improving product quality\nStrong knowledge of software system design, C++ and Python\nPreferred\nStrong software testing experience with a proven track record in scaling highly technical teams\nKnowledge of UNIX/Linux and Windows environments\nKnowledge of neural network architecture and ML/AI deep learning principles\nPrior experience in designing and developing test automation for HW systems involving ASICs or FPGAs\nPrior experience working with live hardware systems and debug tools operating in a real time environment such as networking devices or live computing systems\nPreferred Disciplines:: Computer Engineering\nComputer Science\nEngineering Science (Electrical and Computer)\nEngineering Science (Infrastructure)\nEngineering Science (Machine Intelligence)\nEngineering Science (Robotics)\nAll Co-op programs:: No\nTargeted Co-op Programs:: Targeted Programs\nProfessional Experience Year Co-op (12 - 16 months)\nApplication Deadline:: Nov 1, 2023 11:59 PM\nApplication Receipt Procedure:: Online via system\nIf by Website, go to:: https://www.cerebras.net/careers/?gh_jid=5321500003\nAdditional Application Information:: Please apply with\nboth resume & transcript.\nLacking transcript will disqualify you from being considered.\nNote that applications will be considered on a rolling basis. Apply as early as possible.\nNote to PEY Co-op applicants: In addition to your application by email/website, please ensure that you select the ‚ÄúI intend to apply for this position‚Äù tab on the portal. ¬†This will give us a record of your submitted application in the event that you will be invited for interviews.\nU of T Job Coordinator:: Yasmine Abdelhady\nOrganization:: Cerebras Systems\nDivision:: Computer Science\nWebsite:: https://cerebras.net/\nLength of Workterm:: FLEXIBLE PEY Co-op: 12-16 months (range)\n```\n:::\n\n:::\n::::\n:::\n\n### Finetuning data extraction\n\nThere's a few nuances to the data extraction that mean this simple script needs *just* a bit more extending so it can properly parse the entire local dataset. I'm getting the text corresponding to any inline links, but the links themselves aren't included since they're within the html `<a>` tags, so I need to add handling for those as well. Another nuance is the fact that the formatting for HTML pages has changed[^attimes] (several times actually) over the course of the last two semesters. Since the location of the data has always remained in tables, that's largely a non-issue. While the job title and company name included in the header above any table on job posting pages are currently missed by the script, that data is also present in the tables below (and extracted by the script properly), so that, too, is a non-issue. With one exception: job IDs aren't extracted. Luckily, I had the foresight to configure SingleFile to include the job ID automatically as part of the filename for each HTML page back when I started the archival process, so I can add some logic to parse that as well. Unluckily, however, there were a few periods of time where that configuration was lost[^eglost], so I'm going to have to do some file contents parsing regardless.\n\n[^eglost]: At one point, all of my extensions were uninstalled *somehow* (probably due to a Chrome update gone wrong), and so I had forgotten to reconfigure SingleFile to include job IDs in the saved filenames for many ways until I got some free time and was able to pore over my notes (and reconfigure everything back to the way it was).\n\n[^attimes]: At times purposefully by myself (e.g. adding Job ID and later Job Title to filenames for saved HTML files for easier file browsing and duplication checking), and at times by the university (i.e. there was a big redesign of the job board that took place in the latter half of the fall semester, which made it so the PEY job board uses the same frontend design as job boards on CLNx, whereas prior to that it looked a bit different despite functionally working the same).\n\nA quick manual parsing of the different HTML files shows that there's only two different locations[^foroldandnew] that job IDs can be located, so I can just add another function to try and find the job ID located at either location in all files and I'm now able to extract all the job IDs as well:\n\n[^foroldandnew]: One for the old design, and the other for the new design.\n\n::: {style=\"font-size: 75%;\"}\n```{.python filename=\"parse.py\" .code-overflow-scroll}\nimport argparse\nfrom bs4 import BeautifulSoup\nimport re #<<\n\ndef extract_job_id_from_html(soup): #<<\n    # try to find job ID in a <h1> tag with the specific class #<<\n    header_tag = soup.find('h1', class_='h3 dashboard-header__profile-information-name mobile--small-font color--font--white margin--b--s') #<<\n    if header_tag: #<<\n        header_text = header_tag.get_text(strip=True) #<<\n        match = re.match(r'^(\\d+)', header_text) #<<\n        if match: #<<\n            return match.group(1) #<<\n\n    # if not found, try to find an <h1> tag containing the words \"Job ID\" #<<\n    job_id_tag = soup.find('h1', string=re.compile(r'Job ID', re.IGNORECASE)) #<<\n    if job_id_tag: #<<\n        job_id_text = job_id_tag.get_text(strip=True) #<<\n        match = re.search(r'Job ID\\s*:\\s*(\\d+)', job_id_text, re.IGNORECASE) #<<\n        if match: #<<\n            return match.group(1) #<<\n\n    return None #<<\n\ndef parse_html_file(filepath, verbose=False):\n    with open(filepath, 'r', encoding='utf-8') as file:\n        html_content = file.read()\n\n    soup = BeautifulSoup(html_content, 'lxml')\n\n    data = {}\n    job_id = extract_job_id_from_html(soup) #<<\n    if job_id: #<<\n        data['Job ID'] = job_id #<<\n\n    rows = soup.find_all('tr')  # find all table rows\n\n    for row in rows:\n        tds = row.find_all('td')  # find all table data cells\n\n        if len(tds) >= 2:\n            label_td = tds[0]\n            label_text = '\\n'.join(label_td.stripped_strings)\n\n            value_td = tds[1]\n            value_text = '\\n'.join(value_td.stripped_strings)\n\n            data[label_text] = value_text\n\n    return data\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Parse HTML for 2 column table data.\")\n    parser.add_argument(\"-f\", \"--filepath\", required=True, help=\"Path to the HTML file to be parsed.\")\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Print parsed data.\")\n\n    args = parser.parse_args()\n    data = parse_html_file(args.filepath, args.verbose)\n\n    if args.verbose:\n        for key, value in data.items():\n            print(f\"{key}: {value}\")\n    else:\n        print(\"Parsing completed.\")\n```\n:::\n\nNow with a method of easily extracting all the relevant data from each HTML page, all that's left is to automate running the parser across all files saved within timestamped subdirectories on my local computer and pipe the data into a SQLite DB.\n\n## Storing data in SQLite\n\n### Why SQLite?\n\nSQLite sits right there in the sweet middle spot between raw data formats (like CSV, JSON) that are good fits for simple data (e.g. temperature, word lists) but aren't as great for larger datasets with more complexity (especially when it comes to data analysis) and other larger RDBMS[^RDBMS] libraries that might be better designed for scaleability but are really just overkill for the little pet project that I have here at hand.\n\n[^RDBMS]: relational database management system\n\nAnd the fact that SQLite is a single file on disk means that sharing the finely extracted and processed data is a lot easier too, which is important for me because I want future PEY co-op students to learn what they can from the dataset and better set their expectations for what to expect from program (and I hope the data helps prospective UofT students better make their admission decisions as well).\n\nMost importantly, SQLite is serverless (unlike PostgreSQL or MySQL), which saves me a lot of headache and setup[^especially] for this relatively small-scale project.\n\n[^especially]: Especially since `sqlite3` is built into Python's standard library!\n\n### Pipelining data from ~2k HTML files to a single SQLite DB\n\nUsing SQLite with Python via `sqlite3` is simple enough[^tutorial]. All I need to do is add some additional code for extracting the job posting date from the parent folder for each job posting's HTML page and for iterating across every subfolder for PEY job postings on my local computer, as well as draft up a schema for the SQLite DB and write some code for piping data from Python dictionaries to the DB file.\n\n[^tutorial]: <https://www.digitalocean.com/community/tutorials/how-to-use-the-sqlite3-module-in-python-3>\n\n#### Database Schema\n\nThankfully, all job postings share largely[^clincher] the same fields, so (for the time being[^later]) the schema just ends up being an amalgamation of all the relevant table fields:\n\n[^clincher]: One small slight is that `Application Receipt Procedure` and `Application Method` are often used interchangeably (`Application Receipt Procedure` largely *before* the redesign, and `Application Method` *after*), and when one field is used the other is usually left blank (except for some positions which use both, meaning that I can't exactly combine them into a single field without losing data; e.g. sometimes `Application Method` for a posting states to use the built-in applicant tracking system while `Application Receipt Procedure` states to use some company link, which is likely just attributable to job posters sticking with the default options when creating a new post). It's largely a non-issue, and after some further exploration into the specific overlapping data I should be able to easily combine both into a single field.\n\n[^later]: I could potentially extend the schema with [functional dependencies](https://en.wikipedia.org/wiki/Functional_dependency) for repetitive data like company names, although it's really only `company` and maybe a few other fields where values are guaranteed to repeat (`Job Location Type`, `Salary`, etc. can vary wildly and use combinations of text or integers, so there's not much we can do to unify them) so it's not the best representation for a general-purpose database (lossy conversions can always be made for analysis).\n\n```{.python filename=\"parse_to_db.py\"}\n# ...\n\ndef store_data_in_db(data, db_cursor):\n    columns = ', '.join([f'\"{key}\"' for key in data.keys()])\n    placeholders = ', '.join(['?' for _ in data.values()])\n    sql = f'INSERT INTO \"JobPosting\" ({columns}) VALUES ({placeholders})'\n    try:\n        db_cursor.execute(sql, tuple(data.values()))\n    except sqlite3.IntegrityError:\n        logging.info(\"Integrity Error: Skipping row\")\n        pass\n\ndef create_db_schema(db_cursor):\n    db_cursor.execute('''\n    CREATE TABLE IF NOT EXISTS JobPosting ( #<<\n        id INTEGER, #<<\n        postingDate DATE, #<<\n        title TEXT, #<<\n        company TEXT, #<<\n        companyDivision TEXT, #<<\n        companyWebsite TEXT, #<<\n        location TEXT, #<<\n        locationType TEXT, #<<\n        numPositions INTEGER, #<<\n        salary TEXT, #<<\n        startDate TEXT, #<<\n        endDate TEXT, #<<\n        function TEXT, #<<\n        description TEXT, #<<\n        requirements TEXT, #<<\n        preferredDisciplines TEXT, #<<\n        applicationDeadline TEXT, #<<\n        applicationMethod TEXT, #<<\n        applicationReceiptProcedure TEXT, #<<\n        applicationDetails TEXT, #<<\n        PRIMARY KEY(id, postingDate) #<<\n    ) #<<\n    ''')\n\nif __name__ == \"__main__\":\n    # ...\n```\n\n#### Parsing to `.db`\n\nFinally, with some additional code for properly parsing through all subfolders in my local directory and setting the `postingDate` value based on the folder name[^timestamped] for each file, I can transform the entire dataset of >1.8k job postings into a single `.db` file under 10MB in size in ~3 min[^anditdd].\n\n[^anditdd]: Which'd probably take even less time if it weren't running on an old spinning hard drive.\n\n::: {style=\"font-size: 75%;\"}\n```{.python filename=\"parse_to_db.py\"}\nimport argparse\nfrom bs4 import BeautifulSoup\nimport re\nimport os\nimport sqlite3\nfrom tqdm import tqdm\nimport logging\n\ndef extract_job_id_from_html(soup):\n    # Try to find job ID in a <h1> tag with the specific class\n    header_tag = soup.find('h1', class_='h3 dashboard-header__profile-information-name mobile--small-font color--font--white margin--b--s')\n    if header_tag:\n        header_text = header_tag.get_text(strip=True)\n        match = re.match(r'^(\\d+)', header_text)\n        if match:\n            return match.group(1)\n\n    # If not found, try to find an <h1> tag containing the words \"Job ID\"\n    job_id_tag = soup.find('h1', string=re.compile(r'Job ID', re.IGNORECASE))\n    if job_id_tag:\n        job_id_text = job_id_tag.get_text(strip=True)\n        match = re.search(r'Job ID\\s*:\\s*(\\d+)', job_id_text, re.IGNORECASE)\n        if match:\n            return match.group(1)\n\n    return None\n\ndef parse_html_file(filepath, job_posting_date, verbose=False):\n    with open(filepath, 'r', encoding='utf-8') as file:\n        html_content = file.read()\n\n    soup = BeautifulSoup(html_content, 'lxml')\n\n    # Extract the year, month, and day from the job_posting_date string\n    posting_date = job_posting_date.split('_')[0]\n    data = {'postingDate': posting_date}\n    job_id = extract_job_id_from_html(soup)\n    if job_id:\n        data['id'] = job_id\n\n    rows = soup.find_all('tr')  # find all table rows\n\n    for row in rows:\n        tds = row.find_all('td')  # find all table data cells\n\n        if len(tds) >= 2:\n            label_td = tds[0] #<<\n            label_text = '\\n'.join(label_td.stripped_strings).replace(':', '') #<<\n\n            value_td = tds[1] #<<\n            value_text = '\\n'.join(value_td.stripped_strings) #<<\n\n            links = value_td.find_all('a') #<<\n            for link in links: #<<\n                url = link.get('href') #<<\n                link_text = link.get_text() #<<\n                value_text = value_text.replace(link_text, f'{link_text} ({url})') #<<\n\n            # Map label_text to corresponding database column #<<\n            column_mapping = { #<<\n                # 'Job ID': 'id', #<<\n                # 'Job Posting Date': 'postingDate', #<<\n                'Job Title': 'title', #<<\n                'Organization': 'company', #<<\n                'Division': 'companyDivision', #<<\n                'Website': 'companyWebsite', #<<\n                'Job Location': 'location', #<<\n                'Job Location Type': 'locationType', #<<\n                'Number of Positions': 'numPositions', #<<\n                'Salary': 'salary', #<<\n                'Start Date': 'startDate', #<<\n                'End Date': 'endDate', #<<\n                'Job Function': 'function', #<<\n                'Job Description': 'description', #<<\n                'Job Requirements': 'requirements', #<<\n                'Preferred Disciplines': 'preferredDisciplines', #<<\n                'Application Deadline': 'applicationDeadline', #<<\n                'Application Method': 'applicationMethod', #<<\n                'Application Receipt Procedure': 'applicationReceiptProcedure', #<<\n                'If by Website, go to': 'applicationReceiptProcedure', #<<\n                'Additional Application Information': 'applicationDetails', #<<\n            } #<<\n\n            # Check if label_text matches any of the predefined columns #<<\n            if label_text in column_mapping: #<<\n                db_column = column_mapping[label_text] #<<\n                # If key already exists, append the value to it #<<\n                if db_column in data: #<<\n                    data[db_column] += f'\\n{value_text}' #<<\n                else: #<<\n                    data[db_column] = value_text #<<\n\n    return data\n\ndef store_data_in_db(data, db_cursor):\n    columns = ', '.join([f'\"{key}\"' for key in data.keys()]) #<<\n    placeholders = ', '.join(['?' for _ in data.values()]) #<<\n    sql = f'INSERT INTO \"JobPosting\" ({columns}) VALUES ({placeholders})' #<<\n    try:\n        db_cursor.execute(sql, tuple(data.values()))\n    except sqlite3.IntegrityError:\n        logging.info(\"Integrity Error: Skipping row\")\n        pass\n\ndef create_db_schema(db_cursor):\n    db_cursor.execute('''\n    CREATE TABLE IF NOT EXISTS JobPosting (\n        id INTEGER,\n        postingDate DATE,\n        title TEXT,\n        company TEXT,\n        companyDivision TEXT,\n        companyWebsite TEXT,\n        location TEXT,\n        locationType TEXT,\n        numPositions INTEGER,\n        salary TEXT,\n        startDate TEXT,\n        endDate TEXT,\n        function TEXT,\n        description TEXT,\n        requirements TEXT,\n        preferredDisciplines TEXT,\n        applicationDeadline TEXT,\n        applicationMethod TEXT,\n        applicationReceiptProcedure TEXT,\n        applicationDetails TEXT,\n        PRIMARY KEY(id, postingDate)\n    )\n    ''')\n\nif __name__ == \"__main__\":\n    logging.basicConfig(filename='run.log', level=logging.INFO, format='%(asctime)s %(message)s') #<<\n\n    parser = argparse.ArgumentParser(description=\"Parse HTML files in a folder and store data in SQLite DB.\") #<<\n    parser.add_argument(\"-d\", \"--directory\", default=os.getcwd(), help=\"Path to the directory containing HTML  #<<files. Default is the current directory.\") #<<\n    parser.add_argument(\"--db\", default=os.path.join(os.getcwd(), \"job_postings.db\"), help=\"SQLite database  #<<file to store the parsed data. Default is 'job_postings.db' in the directory specified by -d.\") #<<\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"logging.info parsed data.\") #<<\n\n    args = parser.parse_args() #<<\n\n    conn = sqlite3.connect(args.db) #<<\n    cursor = conn.cursor() #<<\n    create_db_schema(cursor) #<<\n\n    # Get the list of files #<<\n    files = [os.path.join(dirpath, file) for dirpath, _, files in os.walk(args.directory) for file in files if file.endswith('.html') or file.endswith('.htm')] #<<\n\n    # Create a progress bar #<<\n    with tqdm(total=len(files)) as pbar: #<<\n        for subdir, _, files in os.walk(args.directory): #<<\n            job_posting_date = os.path.basename(subdir) #<<\n            for file in files: #<<\n                if file.endswith('.html') or file.endswith('.htm'): #<<\n                    filepath = os.path.join(subdir, file) #<<\n                    logging.info(filepath) #<<\n                    data = parse_html_file(filepath, job_posting_date, args.verbose) #<<\n                    store_data_in_db(data, cursor) #<<\n                    # Update the progress bar #<<\n                    pbar.update(1) #<<\n\n    conn.commit() #<<\n    conn.close() #<<\n    logging.info(\"Parsing and storing completed.\") #<<\n```\n:::\n\n![Parsing through the dataset](https://raw.githubusercontent.com/tqdm/tqdm/blob/74722959a8626fd2057be03e14dcf899c25a3fd5/images/tqdm.gif)\n\n[^timestamped]: e.g. `üìÅ2023-09-17_20-14-10`; easily achievable thanks to [github.com/sadmanca/ahk-scripts/blob/master/keys.ahk](https://github.com/sadmanca/ahk-scripts/blob/master/keys.ahk#L54)\n\n# Viewing the data\n\nAnd so we have it: a single database file storing every single job posted on the UofT PEY Co-op job board for 2T5s[^2t5] (from 2023 to 2024). You can play around with querying the data below[^anotebaout], or download the SQLite `.db` file from the latest release at [github.com/sadmanca/uoft-pey-coop-job-postings](https://github.com/sadmanca/uoft-pey-coop-job-postings).\n\n[^2t5]: UofT Engineering students and alumni often include their graduation year after their name to denote their class. If you start first year in 2021, you will be part of the class of 2025, denoted as ‚Äú2T5‚Äù (pronounced ‚Äútwo-tee-five‚Äù).\n\n[^anotebaout]: I've built-in a SQL view called `JobPostings` (the table with the actual data is called `JobPosting`, without the \"s\") that excludes some of the columns with very long values (e.g. `description`, `requirements`, `preferredDisciplines`, `applicationDetails`) to make table formatting look a bit better if you're running `SELECT *` queries. To run queries with the excluded columns, use the `JobPosting` table instead.\n\n```{.sql .interactive .job_postings}\nSELECT * FROM JobPostings LIMIT 5;\n```\n\n## Sample Queries\n\n### Which companies posted the most number of jobs?\n\n```{.sql .interactive .job_postings}\nSELECT company, COUNT(*) as num_postings\nFROM JobPosting\nGROUP BY company\nORDER BY num_postings DESC;\n```\n\n\n```{=html}\n<p style=\"margin-bottom:5pt;\"></p>\n```\n\n\n### Where are most jobs located?\n\n```{.sql .interactive .job_postings}\nSELECT location, COUNT(*) as num_postings\nFROM JobPosting\nGROUP BY location\nORDER BY num_postings DESC;\n```\n\n\n\n```{=html}\n<p style=\"margin-bottom:5pt;\"></p>\n```\n\n\n### How many job postings are there for mechatronics-adjacent positions?\n\n```{.sql .interactive .job_postings}\nSELECT COUNT(*) AS num_mech_postings\nFROM JobPosting\nWHERE title LIKE '%mechatronic%'\nOR company LIKE '%mechatronic%'\nOR companyDivision LIKE '%mechatronic%'\nOR function LIKE '%mechatronic%'\nOR description LIKE '%mechatronic%'\nOR requirements LIKE '%mechatronic%'\nOR preferredDisciplines LIKE '%mechatronic%';\n```\n\n\n```{=html}\n<p style=\"margin-bottom:5pt;\"></p>\n```\n\n\n### What does a job for a mechatronics-adjacent position look like?\n\n```{.sql .interactive .job_postings}\nSELECT *\nFROM JobPosting\n    WHERE title LIKE '%mechatronic%'\n    OR company LIKE '%mechatronic%'\n    OR companyDivision LIKE '%mechatronic%'\n    OR companyWebsite LIKE '%mechatronic%'\n    OR location LIKE '%mechatronic%'\n    OR locationType LIKE '%mechatronic%'\n    OR salary LIKE '%mechatronic%'\n    OR startDate LIKE '%mechatronic%'\n    OR endDate LIKE '%mechatronic%'\n    OR function LIKE '%mechatronic%'\n    OR description LIKE '%mechatronic%'\n    OR requirements LIKE '%mechatronic%'\n    OR preferredDisciplines LIKE '%mechatronic%'\n    OR applicationDeadline LIKE '%mechatronic%'\n    OR applicationMethod LIKE '%mechatronic%'\n    OR applicationReceiptProcedure LIKE '%mechatronic%'\n    OR applicationDetails LIKE '%mechatronic%'\nLIMIT 1;\n```\n\n***More to come in part 2!***\n\n## Next Steps\n\nYou'll notice that the data still needs some cleaning[^example], which is fine, should be fairly simple to do by manually parsing through the dataset and aggregating similar values.\n\n[^example]: e.g. `Toronto` and `Toronto, ON` are present as two distinct locations, when really they should just be one.\n\nBut after some basic data cleaning[^part2] is the fun part: **analyzing the data to generate some insights on posted jobs**, including...\n\n[^part2]: in part 2 of this series!\n\n- *How many jobs are posted for chemical/mechanical/mineral/... engineers?*\n- *During what periods are the bulk of jobs posted?*\n- *When do specific companies post most/all their jobs?*\n- *How many jobs are reposted[^yeahidkaboutthiseither], and which ones?*\n- ...and more! Feel free to suggest exploration ideas in the comments below.\n\n[^yeahidkaboutthiseither]: Something the Engineering Career Centre (ECC) people who run the PEY Co-op job board don't inform you about ahead of time, employers can *\"repost\"* a job with the same job id (usually several weeks) after the original which makes it'll show up under `New Posting Since Last Login` even if you've already viewed (and even though it's technically still the same single job posting). It's not commonplace but it did occur for a few dozen jobs (I imagine for those where the position was left unfilled after some initial batch of students applied and it was reposted to reinvigorate interest).\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}